*2019/1/10*

# 《Soccer on Your Tabletop》
一篇來自 UW、Google、Facebook 研究人員的論文，拿到 2018 CVPR 的 poster。

### 簡介 Introduction
以往想要由不同視角來觀看球賽，必須使用多個同步的攝影機來重建球場與球員的 3D 場景。本篇想解決的問題是如何只透過一部 monocular 的影片，就 reconstruct  出一場 3D 的足球比賽。

### 深度預測 Depth map estimation
首先是 training data 的取得，作者認為一般的 dataset 包含許多的 pose 和 clothing，但以這項 work 來說，最好的 training data 應該是針對足球員的各種動作、以及使用常見的足球比賽視角。

因此，作者從 Electronic Arts FIFA 遊戲中擷取出 color 和 depth 的資訊，利用這些資訊來訓練 depth estimation network，這個 network 主要由八個 hourglass module 組成，input 吃進一張 RGB 照片及此照片的 segmentation mask ( mask 是後面步驟得到的而非 ground truth，作者說這樣效果比較好 ) ，output 就是深度圖。

深度圖中的深度是 quantized 的，分為 50 個 class，有一個 class 屬於 background，其餘每個 class 間隔 0.02 公尺。這樣做的好處是所有 training images 都有同樣的基準 ( 調整圖片使得第 25 個 class 的深度相同之類 ) 。

### 重建比賽 Reconstructing the Game
主要將整個 pipeline 分為五個步驟：

#### 第一步：Camera Pose Estimation
作者想到的方法是對於每個 frame 都取出 edge points，再建構出一個 distance map 來儲存每個 pixel 與它最靠近的 edge point 之間的距離平方，也就是說對於一組不同的 camera parameter ( 像是焦距、旋轉角度等 ) ，這個 distance map 都會不同，藉由最小化這個 map 裡所有值的總和，即可得出最佳的一組 camera parameter。

#### 第二步：Player Detection and Tracking
首先使用 Faster-RCNN 找出 bounding boxes，再藉由 convolutional pose machine 得到的 keypoints 去 refine，最後藉由這些 bounding box 來產生 tracks。其中，產生 tracks 的方法和我這個 repo 介紹的第一篇論文很像，就是當兩個 track 之間的距離太近時就 merge 起來 ( 本篇用來算距離的是 neck 的位置 ) 。

#### 第三步：Temporal Instance Segmentation
本步驟利用兩種方法來求得較為精準的 segmentation mask。

*( 第一種 )*
先定義 energy 如下圖：
（圖片）
其中 o_p 代表某個 pixel 的 o 值、q 代表這個 pixel 鄰近的 pixel、o_q 代表鄰近 pixel 的 o 值、w_pq 代表兩 pixel 的相似程度 ( 使用 color image 和 edge image 定義 ) 。額外給出幾種 pixel 的 o 值幫助優化 : 我們正在 tracking 的 player 的 keypoints，o_s 定義為 0；其他 player 的 keypoints，o_r 定義為 2；非常可能是 background 的 pixel，o_b 定義為 1。最小化 E 值後，將 o 值 0.5 設為 threshold，即可得出哪些 pixel 屬於我們關注的 player ，也就是 segmentation mask 了。

然而這樣的 mask 方法容易把 background 算成 player，因此以下有第二種方法。

*( 第二種 )*
對每個 frame 而言，取 15 個 frame 作為一個 volume，對齊 neck keypoint，解 dense CRF 後可得每個 frame 的 mask。

第二種方法能夠分隔出 player 與 background，但對於旁邊有其他 player 的表現就稍差一些。因此作者將兩種方法的 mask 結合起來，作為最後的結果 ( 這個 mask 資訊會被拿去訓練前面提到的 depth estimation network ) 。

#### 第四步：Mesh Generation
利用第三步得到的 mask 和原圖訓練出 depth estimation network，吐出的 distance map 再藉由第一步得到的 camera parameter 投射到共同的空間中，此時已獲得每個球員的 3D point-cloud。

#### 第五步：Trajectories in 3D
由於相機位置、bounding boxes 可能不精確，我們需要對 trajectory 進行 smoothing 以獲得最好表現。作者提出的方法是最小化以下式子：
（圖片）
其中 X_t 是時間 t 時，player 的 3D trajectory；D_t 是時間 t 時，bounding box 在 3D 中的位置中心。第一項用來確保此投射不會離最原始的 detection 太遠；第二項用來確保時間上的平滑程度足夠。

### 實驗 Experiments
首先是比較不同方法在 FIFA 提取出的 dataset 中，預測深度及 3D mesh 的表現：
（圖片）
上圖可看出本文的方法已經非常接近 ground truth。

接著是在 YouTube 影片上的表現比較：
（圖片）

最後是同一個時間點，不同視角去做深度預測：
（圖片）
上圖中不同顏色代表從不同視角預測出的結果，可以看出每個視角的結果大致吻合，不會因為視角改變而有很大落差。

### 結論 Conclusion
作者提到許多論文的弱點，像是跳躍在空中的 player 無法被重建、足球無法被重建、被遮擋住的 player 有可能消失、以及最重要的一點：如果想使用 AR device 來觀看 live 比賽，real-time 是很重要的研究方向。

### 附錄 Appendix
Project page: http://grail.cs.washington.edu/projects/soccer/
Github: https://github.com/krematas/soccerontable
arXiv: https://arxiv.org/abs/1806.00890
