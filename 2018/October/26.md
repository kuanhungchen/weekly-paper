**2018/10/26**

# 《Detect-and-Track: Efficient Pose Estimation in Videos》
<p><img src="https://i.imgur.com/ITnhy0o.png" width=600 /></p>  

這篇有拿到 CVPR2018 的 poster，並且在 ICCV 2017 的 PoseTrack Challenge 中獲得了 keypoint tracking task 的排名第一。  

### 簡介 ###

這篇主要想解決的問題是如何在複雜且多人的影片中對人的 body keypoint 做 tracking。文中提到以往的方法通常是 two-stage，第一階段是先用 frame-level 的 pose estimatior 找出人的姿勢，第二階段則是對數個frame中的人去做linking。但是影片畢竟和影像不同，於是這篇提出了一個改良的方法：使用 3D 的 conv layer 去保留住時間上的資訊。經過作者的實驗後，證實這樣的作法能夠比 2D baseline 的方法有更好的表現。

### 模型 ###
<p><img src="https://i.imgur.com/THQY0aR.png" /></p>

這篇的作法大致可分為兩個階段：  

第一階段中，首先他們使用了 3D 版本的 Mask R-CNN，即：每個 convolutions 都從 2D 改為 3D ，並且他們將原先 pretrain 好的參數放在 3D kernel 的最中間，其他層則設為零。這個網路會提取出原影片的 feature map（也是 3D 的）。  
接著，會將上述網路的 output 丟進一個叫做 Tube Proposal Network (TPN) 中，這個網路和 Mask R-CNN 的 RPN 類似，但它選出的不是 Region，而是一個一個的 tube anchor，這些 tube anchor 會有自己的 classification 和 regression prediction，分別象徵了這個 tube 是否包含 human。  
最後，這個 tube 會被切成 T 張 slice（假設影片長度為 T ），並且使用 Mask R-CNN 中的 ROIAlign，找出在 feature map 中對應的位置，然後將這一張張對應位置組成一個 RxRxT 的 feature map，丟進 classification head 和 keypoint head，其中的 keypoint head 是採用了 8 個 3D con layers 和 2 個 deconvolutional layers，最後出來的會是每張 frame 的 keypoint heatmap。  

第二階段是對每個 frame 中的每個 box，連接到下一個 frame 中的每個 box，box 之間的 edges 有它們各自的 cost，越不像的 box，它們之間的 cost 就會越高，非常直觀。但是要如何定義「像不像」呢？文中提出了三個方法，分別是 1) 用 CNN 取出每個 box 的 feature，算出它們之間的 cosine distance，這個值越大表示這兩個 box 越不像。 2) 取一個 ratio 是這兩個 box 在位置上的「交集除以聯集」，也就是說，box 的位置越接近，這兩個 box 就會被定義為越像。 3) 計算兩個 box 中 pose 的 PCKh distance，其中的 PCKh 是其他篇 paper 所設計出來的，用來衡量 pose 之間的相似度。  
經過實驗後發現，三種 criterion 的 performance 都相去不遠，但是第二種的計算比較簡單，因此他們就使用 box overlap 作為最後的 model。  

### 結果 ###

文中提到，由於硬體上的限制，作者只能測試較為低解析度的影片，Mask R-CNN 中的 ResNet 也只能選擇 18 層的版本。然而儘管如此，作者發現這樣做的結果仍然比他們所設計的 baseline model（基本上是 2D 的 Mask R-CNN，差別在於是否在最一開始就保留時間上的資訊）還要好。因此他們相信如果使用更好的硬體設備，那麼基於這套 3D Mask R-CNN 的方法一定能有很好的表現，特別是在高解析度的 input 和高 capacity 的 model。  

### 附錄 ###

Project Page: https://rohitgirdhar.github.io/DetectAndTrack/  
Code: https://github.com/facebookresearch/DetectAndTrack/  
arxiv: https://arxiv.org/abs/1712.09184
